{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Run the following experiment:\n",
    "\n",
    "1. Define a prompt and use Lumina 2.0 to generate two images from the prompt\n",
    "2.\n",
    "    (a) Use Qwen 2.5 VL 7B to describe the two images in details\n",
    "    (b) Use Qwen 2.5 VL 7B to generate a description that would be correct for both the images. Ask it to Include as much information as possible in the description\n",
    "    (c) For each of the generated image ask Qwen to describe the contents/activities in the image that is not present in the other image.\n",
    "3. Using original prompt as the reference and output of 2a, 2b, 2c(i, ii) as the candidates. Compute BertScore of each pair (reference, candidate) and display a table of precision, recall, and f1 score nicely for all cases.\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BertScore\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from diffusers import DiffusionPipeline\n",
    "import pandas as pd\n",
    "from bert_score import score as bert_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def generate_images_with_lumina(prompt, num_images=2):\n",
    "    \"\"\"Generate images using Lumina 2.0\"\"\"\n",
    "    print(f\"Generating {num_images} images from prompt: {prompt}\")\n",
    "    \n",
    "    # Load Lumina 2.0 model\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        \"SimianLuo/LuminaLite_v2\", \n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    # Generate images\n",
    "    images = []\n",
    "    for i in range(num_images):\n",
    "        image = pipe(prompt, num_inference_steps=30).images[0]\n",
    "        images.append(image)\n",
    "        # Save the generated image\n",
    "        image.save(f\"generated_image_{i+1}.png\")\n",
    "        print(f\"Image {i+1} generated and saved as generated_image_{i+1}.png\")\n",
    "    \n",
    "    return images\n",
    "\n",
    "def load_qwen_vl():\n",
    "    \"\"\"Load Qwen 2.5 VL 7B model\"\"\"\n",
    "    processor = LlavaProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B\")\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        \"Qwen/Qwen2.5-VL-7B\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def get_image_description(model, processor, image, prompt):\n",
    "    \"\"\"Get a description of an image using Qwen VL\"\"\"\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    description = processor.decode(output[0], skip_special_tokens=True)\n",
    "    # Clean up the output (extract just the model's response)\n",
    "    if \"assistant\" in description.lower():\n",
    "        description = description.split(\"assistant:\", 1)[1].strip()\n",
    "    \n",
    "    return description.strip()\n",
    "\n",
    "def detailed_image_description(model, processor, image):\n",
    "    \"\"\"Get detailed description of an image\"\"\"\n",
    "    prompt = \"Please describe this image in detail.\"\n",
    "    return get_image_description(model, processor, image, prompt)\n",
    "\n",
    "def common_description(model, processor, image1, image2):\n",
    "    \"\"\"Get a description that applies to both images\"\"\"\n",
    "    # Using a grid of both images\n",
    "    combined_image = Image.new('RGB', (image1.width + image2.width, max(image1.height, image2.height)))\n",
    "    combined_image.paste(image1, (0, 0))\n",
    "    combined_image.paste(image2, (image1.width, 0))\n",
    "    \n",
    "    prompt = \"Look at these two images. Generate a detailed description that would be correct for both images. Include as much information as possible that applies to both images.\"\n",
    "    \n",
    "    return get_image_description(model, processor, combined_image, prompt)\n",
    "\n",
    "def unique_elements(model, processor, image1, image2):\n",
    "    \"\"\"Describe elements in image1 that are not in image2\"\"\"\n",
    "    # Using a grid of both images\n",
    "    combined_image = Image.new('RGB', (image1.width + image2.width, max(image1.height, image2.height)))\n",
    "    combined_image.paste(image1, (0, 0))\n",
    "    combined_image.paste(image2, (image1.width, 0))\n",
    "    \n",
    "    prompt = \"Look at these two images. The first image is on the left, the second image is on the right. Describe the contents or activities in the first image (left) that are not present in the second image (right).\"\n",
    "    \n",
    "    return get_image_description(model, processor, combined_image, prompt)\n",
    "\n",
    "def calculate_bert_scores(reference, candidates):\n",
    "    \"\"\"Calculate BertScore between reference and candidates\"\"\"\n",
    "    candidate_texts = list(candidates.values())\n",
    "    references = [reference] * len(candidate_texts)\n",
    "    \n",
    "    # Calculate BertScore\n",
    "    P, R, F1 = bert_score(candidate_texts, references, lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"Description\": list(candidates.keys()),\n",
    "        \"Precision\": P.tolist(),\n",
    "        \"Recall\": R.tolist(),\n",
    "        \"F1\": F1.tolist()\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def display_images(images):\n",
    "    \"\"\"Display a list of images\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(10 * len(images), 10))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Image {i+1}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # 1. Define a prompt and generate images\n",
    "    prompt = \"A serene mountain lake at sunset with a small boat\"\n",
    "    images = generate_images_with_lumina(prompt)\n",
    "    \n",
    "    # Display the generated images\n",
    "    display_images(images)\n",
    "    \n",
    "    # 2. Load Qwen 2.5 VL model\n",
    "    model, processor = load_qwen_vl()\n",
    "    \n",
    "    # 2a. Get detailed descriptions of each image\n",
    "    description1 = detailed_image_description(model, processor, images[0])\n",
    "    description2 = detailed_image_description(model, processor, images[1])\n",
    "    \n",
    "    print(\"\\nDetailed description of Image 1:\")\n",
    "    print(description1)\n",
    "    print(\"\\nDetailed description of Image 2:\")\n",
    "    print(description2)\n",
    "    \n",
    "    # 2b. Generate a common description\n",
    "    common_desc = common_description(model, processor, images[0], images[1])\n",
    "    print(\"\\nCommon description for both images:\")\n",
    "    print(common_desc)\n",
    "    \n",
    "    # 2c. Identify unique elements in each image\n",
    "    unique_in_1 = unique_elements(model, processor, images[0], images[1])\n",
    "    unique_in_2 = unique_elements(model, processor, images[1], images[0])\n",
    "    \n",
    "    print(\"\\nUnique elements in Image 1:\")\n",
    "    print(unique_in_1)\n",
    "    print(\"\\nUnique elements in Image 2:\")\n",
    "    print(unique_in_2)\n",
    "    \n",
    "    # 3. Calculate BertScore\n",
    "    candidates = {\n",
    "        \"Image 1 Description\": description1,\n",
    "        \"Image 2 Description\": description2,\n",
    "        \"Common Description\": common_desc,\n",
    "        \"Unique in Image 1\": unique_in_1,\n",
    "        \"Unique in Image 2\": unique_in_2\n",
    "    }\n",
    "    \n",
    "    bert_scores_df = calculate_bert_scores(prompt, candidates)\n",
    "    \n",
    "    print(\"\\nBertScore Results:\")\n",
    "    print(bert_scores_df)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    bert_scores_df.to_csv(\"bert_scores_results.csv\", index=False)\n",
    "    print(\"Results saved to bert_scores_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
